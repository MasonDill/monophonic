{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMUS_PATH = \"/primus_data/\"\n",
    "SAVE_MODEL = \"/models/semantic_1-\"\n",
    "\n",
    "VOCABULARY_PATH = \"/tf-end-to-end/Data/vocabulary_semantic.txt\"\n",
    "WORD_DELIMITER = '\\t'\n",
    "SEMANTIC = True\n",
    "DISTORTIONS = False\n",
    "\n",
    "IMG_HEIGHT = 128\n",
    "MAX_EPOCHS = 100\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import multiprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Training / Testing Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train: ', 35071)\n",
      "('Test: ', 52607)\n",
      "/models/semantic_1-35071\n"
     ]
    }
   ],
   "source": [
    "data_samples = os.listdir(PRIMUS_PATH)\n",
    "#shuffle\n",
    "np.random.shuffle(data_samples)\n",
    "#split data into train and test, 80% train, 20% test\n",
    "VAL_SPLIT = 0.4\n",
    "training_list = data_samples[:int(len(data_samples)*VAL_SPLIT)]\n",
    "validation_list = data_samples[int(len(data_samples)*VAL_SPLIT):]\n",
    "print(\"Train: \", len(training_list))\n",
    "print(\"Test: \", len(validation_list))\n",
    "\n",
    "SAVE_MODEL +=  str(len(training_list))\n",
    "print(SAVE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocabulary\n",
    "set of possible outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int = {}\n",
    "int2word = {}\n",
    "            \n",
    "vocab_file = open(VOCABULARY_PATH,'r')\n",
    "vocab_list = vocab_file.read().splitlines()\n",
    "for word in vocab_list:\n",
    "    if not word in word2int:\n",
    "        word_idx = len(word2int)\n",
    "        word2int[word] = word_idx\n",
    "        int2word[word_idx] = word\n",
    "\n",
    "vocab_file.close()\n",
    "\n",
    "vocabulary_size = len(word2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"img_height\":IMG_HEIGHT,\n",
    "    \"img_width\":None,\n",
    "    \"batch_size\":16,\n",
    "    \"img_channels\":1,\n",
    "    \"conv_blocks\":4,\n",
    "    \"conv_filter_n\":[32,64,128,256],\n",
    "    \"conv_filter_size\":[ [3,3], [3,3], [3,3], [3,3] ],\n",
    "    \"conv_pooling_size\":[ [2,2], [2,2], [2,2], [2,2] ],\n",
    "    \"rnn_units\":512,\n",
    "    \"rnn_layers\":2,\n",
    "    \"vocabulary_size\": vocabulary_size\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed parameters to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(features, alpha=0.2, name=None):\n",
    "  with ops.name_scope(name, \"LeakyRelu\", [features, alpha]):\n",
    "    features = ops.convert_to_tensor(features, name=\"features\")\n",
    "    alpha = ops.convert_to_tensor(alpha, name=\"alpha\")\n",
    "    return math_ops.maximum(alpha * features, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Assert parameters\n",
    "input = tf.placeholder(shape=(None,\n",
    "                               params['img_height'],\n",
    "                               params['img_width'],\n",
    "                               params['img_channels']),  # [batch, height, width, channels]\n",
    "                        dtype=tf.float32,\n",
    "                        name='model_input')\n",
    "input_shape = tf.shape(input)\n",
    "width_reduction = 1\n",
    "height_reduction = 1\n",
    "\n",
    "# Convolutional blocks\n",
    "x = input\n",
    "for i in range(params['conv_blocks']):\n",
    "    x = tf.layers.conv2d(\n",
    "        inputs=x,\n",
    "        filters=params['conv_filter_n'][i],\n",
    "        kernel_size=params['conv_filter_size'][i],\n",
    "        padding=\"same\",\n",
    "        activation=None)\n",
    "\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = leaky_relu(x)\n",
    "    x = tf.layers.max_pooling2d(inputs=x,\n",
    "                                pool_size=params['conv_pooling_size'][i],\n",
    "                                strides=params['conv_pooling_size'][i])\n",
    "\n",
    "    width_reduction = width_reduction * params['conv_pooling_size'][i][1]\n",
    "    height_reduction = height_reduction * params['conv_pooling_size'][i][0]\n",
    "\n",
    "# Prepare output of conv block for recurrent blocks\n",
    "features = tf.transpose(x, perm=[2, 0, 3, 1])  # -> [width, batch, height, channels](time_major=True)\n",
    "feature_dim = params['conv_filter_n'][-1] * (params['img_height'] / height_reduction)\n",
    "feature_width = input_shape[2] / width_reduction\n",
    "features = tf.reshape(features, tf.stack([tf.cast(feature_width,'int32'), input_shape[0], tf.cast(feature_dim,'int32')]))  # -> [width, batch, features]\n",
    "tf.constant(params['img_height'],name='input_height')\n",
    "tf.constant(width_reduction,name='width_reduction')\n",
    "\n",
    "# Recurrent block\n",
    "rnn_keep_prob = tf.placeholder(dtype=tf.float32, name=\"keep_prob\")\n",
    "rnn_hidden_units = params['rnn_units']\n",
    "rnn_hidden_layers = params['rnn_layers']\n",
    "rnn_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "    tf.contrib.rnn.MultiRNNCell(\n",
    "        [tf.nn.rnn_cell.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(rnn_hidden_units),input_keep_prob=rnn_keep_prob)\n",
    "         for _ in range(rnn_hidden_layers)]),\n",
    "    tf.contrib.rnn.MultiRNNCell(\n",
    "        [tf.nn.rnn_cell.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(rnn_hidden_units),input_keep_prob=rnn_keep_prob)\n",
    "         for _ in range(rnn_hidden_layers)]),\n",
    "    features,\n",
    "    dtype=tf.float32,\n",
    "    time_major=True,\n",
    ")\n",
    "\n",
    "rnn_outputs = tf.concat(rnn_outputs, 2)\n",
    "logits = tf.contrib.layers.fully_connected(\n",
    "    rnn_outputs,\n",
    "    params['vocabulary_size'] + 1,  # BLANK\n",
    "    activation_fn=None,\n",
    ")\n",
    "\n",
    "tf.add_to_collection(\"logits\",logits) # for restoring purposes\n",
    "# CTC Loss computation\n",
    "seq_len = tf.placeholder(tf.int32, [None], name='seq_lengths')\n",
    "targets = tf.sparse_placeholder(dtype=tf.int32, name='target')\n",
    "ctc_loss = tf.nn.ctc_loss(labels=targets, inputs=logits, sequence_length=seq_len,time_major=True)\n",
    "loss = tf.reduce_mean(ctc_loss)\n",
    "# CTC decoding\n",
    "decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, seq_len)\n",
    "inputs = input\n",
    "# decoded, log_prob = tf.nn.ctc_beam_search_decoder(logits,seq_len,beam_width=50,top_paths=1merge_repeated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_inputs_to_ctc_format(target_text):\n",
    "    SPACE_TOKEN = '-'\n",
    "    SPACE_INDEX = 4\n",
    "    FIRST_INDEX = 0\n",
    "\n",
    "    original = ' '.join(target_text.strip().lower().split(' ')).replace('.', '').replace('?', '').replace(',', '').replace(\"'\", '').replace('!', '').replace('-', '')\n",
    "    print(original)\n",
    "    targets = original.replace(' ', '  ')\n",
    "    targets = targets.split(' ')\n",
    "\n",
    "    # Adding blank label\n",
    "    targets = np.hstack([SPACE_TOKEN if x == '' else list(x) for x in targets])\n",
    "\n",
    "    # Transform char into index\n",
    "    targets = np.asarray([SPACE_INDEX if x == SPACE_TOKEN else ord(x) - FIRST_INDEX\n",
    "                          for x in targets])\n",
    "\n",
    "    # Creating sparse representation to feed the placeholder\n",
    "    train_targets = sparse_tuple_from([targets])\n",
    "\n",
    "    return train_targets, original\n",
    "\n",
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n] * len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape\n",
    "\n",
    "def sparse_tensor_to_strs(sparse_tensor):\n",
    "    indices= sparse_tensor[0][0]\n",
    "    values = sparse_tensor[0][1]\n",
    "    dense_shape = sparse_tensor[0][2]\n",
    "\n",
    "    strs = [ [] for i in range(dense_shape[0]) ]\n",
    "\n",
    "    string = []\n",
    "    ptr = 0\n",
    "    b = 0\n",
    "\n",
    "    for idx in range(len(indices)):\n",
    "        if indices[idx][0] != b:\n",
    "            strs[b] = string\n",
    "            string = []\n",
    "            b = indices[idx][0]\n",
    "\n",
    "        string.append(values[ptr])\n",
    "\n",
    "        ptr = ptr + 1\n",
    "\n",
    "    strs[b] = string\n",
    "\n",
    "    return strs\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, maxlen=None, dtype=np.float32,\n",
    "                  padding='post', truncating='post', value=0.):\n",
    "    lengths = np.asarray([len(s) for s in sequences], dtype=np.int64)\n",
    "\n",
    "    nb_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if len(s) == 0:\n",
    "            continue  # empty list was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x, lengths\n",
    "\n",
    "\n",
    "def word_separator():\n",
    "    return '\\t'\n",
    "\n",
    "def levenshtein(a,b):\n",
    "    \"Computes the Levenshtein distance between a and b.\"\n",
    "    n, m = len(a), len(b)\n",
    "\n",
    "    if n > m:\n",
    "        a,b = b,a\n",
    "        n,m = m,n\n",
    "\n",
    "    current = range(n+1)\n",
    "    for i in range(1,m+1):\n",
    "        previous, current = current, [i]+[0]*n\n",
    "        for j in range(1,n+1):\n",
    "            add, delete = previous[j]+1, current[j-1]+1\n",
    "            change = previous[j-1]\n",
    "            if a[j-1] != b[i-1]:\n",
    "                change = change + 1\n",
    "            current[j] = min(add, delete, change)\n",
    "\n",
    "    return current[n]\n",
    "\n",
    "\n",
    "def edit_distance(a,b,EOS=-1,PAD=-1):\n",
    "    _a = [s for s in a if s != EOS and s != PAD]\n",
    "    _b = [s for s in b if s != EOS and s != PAD]\n",
    "\n",
    "    return levenshtein(_a,_b)\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "    return (255. - image)/255.\n",
    "\n",
    "\n",
    "def resize(image, height):\n",
    "    width = int(float(height * image.shape[1]) / image.shape[0])\n",
    "    sample_img = cv2.resize(image, (width, height))\n",
    "    return sample_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(pid):\n",
    "    sample_filepath = validation_list[pid]\n",
    "    sample_full_filepath = PRIMUS_PATH + '/' + sample_filepath + '/' +sample_filepath\n",
    "    print(sample_full_filepath)\n",
    "    #Image\n",
    "    sample_img = cv2.imread(sample_full_filepath + '.png', False)\n",
    "    height = params['img_height']\n",
    "    sample_img = resize(sample_img, height)\n",
    "    images[pid] = (normalize(sample_img))\n",
    "        \n",
    "    if SEMANTIC:\n",
    "        sample_full_filepath += '.semantic'\n",
    "    else:\n",
    "        sample_full_filepath += '.agnostic'\n",
    "            \n",
    "    sample_gt_file = open(sample_full_filepath, 'r')\n",
    "    sample_gt_plain = sample_gt_file.readline().rstrip().split(word_separator())\n",
    "    sample_gt_file.close()\n",
    "        \n",
    "    labels.append([word2int[lab] for lab in sample_gt_plain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_COLUMN = 0\n",
    "\n",
    "class static_counter:\n",
    "    def __init__(self, top):\n",
    "        self.value = 0\n",
    "        self.top = top\n",
    "    def __init__(self, value, top):\n",
    "        self.value = value\n",
    "        self.top = top\n",
    "    def incr(self):\n",
    "        self.value = (self.value+1) % (self.top)\n",
    "\n",
    "def get_batch(params, img_counter):\n",
    "    images = []\n",
    "    labels = []\n",
    "    # Read files\n",
    "    for _ in range(params['batch_size']):\n",
    "        sample_filepath = training_list[img_counter.value]\n",
    "        sample_full_filepath = PRIMUS_PATH + '/' + sample_filepath + '/' + sample_filepath\n",
    "        # IMAGE\n",
    "        if DISTORTIONS:\n",
    "            sample_img = cv2.imread(sample_full_filepath + '_distorted.jpg', False) # Grayscale is assumed\n",
    "        else:\n",
    "            sample_img = cv2.imread(sample_full_filepath + '.png', False)  # Grayscale is assumed!\n",
    "        \n",
    "        if sample_img is None:\n",
    "            raise Exception('Error loading sample: ' + sample_full_filepath + '.png')\n",
    "            \n",
    "        height = params['img_height']\n",
    "        sample_img = resize(sample_img,height)\n",
    "        images.append(normalize(sample_img))\n",
    "        # GROUND TRUTH\n",
    "        if SEMANTIC:\n",
    "            sample_full_filepath = sample_full_filepath + '.semantic'\n",
    "        else:\n",
    "            sample_full_filepath = sample_full_filepath + '.agnostic'\n",
    "        \n",
    "        sample_gt_file = open(sample_full_filepath, 'r')\n",
    "        sample_gt_plain = sample_gt_file.readline().rstrip().split(word_separator())\n",
    "        sample_gt_file.close()\n",
    "        labels.append([word2int[lab] for lab in sample_gt_plain])\n",
    "        img_counter.incr()\n",
    "    # Transform to batch\n",
    "    image_widths = [img.shape[1] for img in images]\n",
    "    max_image_width = max(image_widths)\n",
    "    batch_images = np.ones(shape=[params['batch_size'],\n",
    "                                   params['img_height'],\n",
    "                                   max_image_width,\n",
    "                                   params['img_channels']], dtype=np.float32)*PAD_COLUMN\n",
    "    for i, img in enumerate(images):\n",
    "        batch_images[i, 0:img.shape[0], 0:img.shape[1], 0] = img\n",
    "    # LENGTH\n",
    "    width_reduction = 1\n",
    "    for i in range(params['conv_blocks']):\n",
    "        width_reduction = width_reduction * params['conv_pooling_size'][i][1]\n",
    "    lengths = [ batch_images.shape[2] / width_reduction ] * batch_images.shape[0]\n",
    "    return {\n",
    "        'inputs': batch_images,\n",
    "        'seq_lengths': np.asarray(lengths),\n",
    "        'targets': labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValidation(params, validation_dict):\n",
    "    if not (validation_dict is None):\n",
    "        return validation_dict, len(validation_list)\n",
    "    \n",
    "    images = [None] * len(validation_list)\n",
    "    labels = []\n",
    "    processes = [] \n",
    "    with multiprocessing.Pool(processes=len(validation_list)) as pool:\n",
    "        # Use the pool to map the worker function to a range of values\n",
    "        results = pool.map(read_files, range(len(validation_list)))\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    print(\"flag 1\")\n",
    "    #Transform to batch\n",
    "    image_widths = [img.shape[1] for img in images]\n",
    "    max_image_width = max(image_widths)\n",
    "    \n",
    "    batch_images = np.ones(shape=[len(validation_list),\n",
    "                                    params['img_height'],\n",
    "                                    max_image_width,\n",
    "                                    params['img_channels']], dtype=np.float32)*PAD_COLUMN\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        batch_images[i, 0:img.shape[0], 0:img.shape[1], 0] = img\n",
    "    \n",
    "    # LENGTH\n",
    "    width_reduction = 1\n",
    "    for i in range(params['conv_blocks']):\n",
    "        width_reduction = width_reduction * params['conv_pooling_size'][i][1]\n",
    "    \n",
    "    lengths = [ batch_images.shape[2] / width_reduction ] * batch_images.shape[0]\n",
    "    \n",
    "    validation_dict = {\n",
    "        'inputs': batch_images,\n",
    "        'seq_lengths': np.asarray(lengths),\n",
    "        'targets': labels,\n",
    "    }\n",
    "            \n",
    "        \n",
    "    return validation_dict, len(validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    " current_batch_counter = static_counter(0, len(training_list))\n",
    "# validation_dict = None\n",
    "# validation_batch, validation_size = getValidation(params, validation_dict)timeSignature-C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value at epoch 0:909.47\n",
      "Validating...\n",
      "Saving the model...\n",
      "INFO:tensorflow:/models/semantic_1-35071-0 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "------------------------------\n",
      "EPOCH 0 COMPLETE\n",
      "EPOCH 1 COMPLETE\n",
      "EPOCH 2 COMPLETE\n",
      "EPOCH 3 COMPLETE\n",
      "EPOCH 4 COMPLETE\n",
      "EPOCH 5 COMPLETE\n",
      "EPOCH 6 COMPLETE\n",
      "EPOCH 7 COMPLETE\n",
      "EPOCH 8 COMPLETE\n",
      "EPOCH 9 COMPLETE\n",
      "Loss value at epoch 10:118.53384\n",
      "Validating...\n",
      "Saving the model...\n",
      "INFO:tensorflow:/models/semantic_1-35071-10 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "------------------------------\n",
      "EPOCH 10 COMPLETE\n",
      "EPOCH 11 COMPLETE\n",
      "EPOCH 12 COMPLETE\n",
      "EPOCH 13 COMPLETE\n",
      "EPOCH 14 COMPLETE\n",
      "EPOCH 15 COMPLETE\n",
      "EPOCH 16 COMPLETE\n",
      "EPOCH 17 COMPLETE\n",
      "EPOCH 18 COMPLETE\n",
      "EPOCH 19 COMPLETE\n",
      "Loss value at epoch 20:114.56581\n",
      "Validating...\n",
      "Saving the model...\n",
      "INFO:tensorflow:/models/semantic_1-35071-20 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "------------------------------\n",
      "EPOCH 20 COMPLETE\n",
      "EPOCH 21 COMPLETE\n",
      "EPOCH 22 COMPLETE\n",
      "EPOCH 23 COMPLETE\n",
      "EPOCH 24 COMPLETE\n",
      "EPOCH 25 COMPLETE\n",
      "EPOCH 26 COMPLETE\n",
      "EPOCH 27 COMPLETE\n",
      "EPOCH 28 COMPLETE\n",
      "EPOCH 29 COMPLETE\n",
      "Loss value at epoch 30:123.87599\n",
      "Validating...\n",
      "Saving the model...\n",
      "INFO:tensorflow:/models/semantic_1-35071-30 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "------------------------------\n",
      "EPOCH 30 COMPLETE\n",
      "EPOCH 31 COMPLETE\n",
      "EPOCH 32 COMPLETE\n",
      "EPOCH 33 COMPLETE\n",
      "EPOCH 34 COMPLETE\n",
      "EPOCH 35 COMPLETE\n",
      "EPOCH 36 COMPLETE\n",
      "EPOCH 37 COMPLETE\n",
      "EPOCH 38 COMPLETE\n",
      "EPOCH 39 COMPLETE\n",
      "Loss value at epoch 40:122.26464\n",
      "Validating...\n",
      "Saving the model...\n",
      "INFO:tensorflow:/models/semantic_1-35071-40 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "------------------------------\n",
      "EPOCH 40 COMPLETE\n",
      "EPOCH 41 COMPLETE\n",
      "EPOCH 42 COMPLETE\n",
      "EPOCH 43 COMPLETE\n",
      "EPOCH 44 COMPLETE\n",
      "EPOCH 45 COMPLETE\n",
      "EPOCH 46 COMPLETE\n",
      "EPOCH 47 COMPLETE\n",
      "EPOCH 48 COMPLETE\n",
      "EPOCH 49 COMPLETE\n",
      "Loss value at epoch 50:118.01333\n",
      "Validating...\n",
      "Saving the model...\n",
      "INFO:tensorflow:/models/semantic_1-35071-50 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "------------------------------\n",
      "EPOCH 50 COMPLETE\n",
      "EPOCH 51 COMPLETE\n",
      "EPOCH 52 COMPLETE\n",
      "EPOCH 53 COMPLETE\n",
      "EPOCH 54 COMPLETE\n",
      "EPOCH 55 COMPLETE\n",
      "EPOCH 56 COMPLETE\n",
      "EPOCH 57 COMPLETE\n",
      "EPOCH 58 COMPLETE\n",
      "EPOCH 59 COMPLETE\n",
      "Loss value at epoch 60:126.913536\n",
      "Validating...\n",
      "Saving the model...\n",
      "INFO:tensorflow:/models/semantic_1-35071-60 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "------------------------------\n",
      "EPOCH 60 COMPLETE\n",
      "EPOCH 61 COMPLETE\n",
      "EPOCH 62 COMPLETE\n",
      "EPOCH 63 COMPLETE\n",
      "EPOCH 64 COMPLETE\n",
      "EPOCH 65 COMPLETE\n",
      "EPOCH 66 COMPLETE\n",
      "EPOCH 67 COMPLETE\n",
      "EPOCH 68 COMPLETE\n",
      "EPOCH 69 COMPLETE\n",
      "Loss value at epoch 70:114.3124\n",
      "Validating...\n",
      "Saving the model...\n",
      "INFO:tensorflow:/models/semantic_1-35071-70 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "------------------------------\n",
      "EPOCH 70 COMPLETE\n",
      "EPOCH 71 COMPLETE\n",
      "EPOCH 72 COMPLETE\n",
      "EPOCH 73 COMPLETE\n",
      "EPOCH 74 COMPLETE\n",
      "EPOCH 75 COMPLETE\n",
      "EPOCH 76 COMPLETE\n",
      "EPOCH 77 COMPLETE\n",
      "EPOCH 78 COMPLETE\n",
      "EPOCH 79 COMPLETE\n",
      "Loss value at epoch 80:109.75836\n",
      "Validating...\n",
      "Saving the model...\n",
      "INFO:tensorflow:/models/semantic_1-35071-80 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "------------------------------\n",
      "EPOCH 80 COMPLETE\n",
      "EPOCH 81 COMPLETE\n",
      "EPOCH 82 COMPLETE\n",
      "EPOCH 83 COMPLETE\n",
      "EPOCH 84 COMPLETE\n",
      "EPOCH 85 COMPLETE\n",
      "EPOCH 86 COMPLETE\n",
      "EPOCH 87 COMPLETE\n",
      "EPOCH 88 COMPLETE\n",
      "EPOCH 89 COMPLETE\n",
      "Loss value at epoch 90:120.63327\n",
      "Validating...\n",
      "Saving the model...\n",
      "INFO:tensorflow:/models/semantic_1-35071-90 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "------------------------------\n",
      "EPOCH 90 COMPLETE\n",
      "EPOCH 91 COMPLETE\n",
      "EPOCH 92 COMPLETE\n",
      "EPOCH 93 COMPLETE\n",
      "EPOCH 94 COMPLETE\n",
      "EPOCH 95 COMPLETE\n",
      "EPOCH 96 COMPLETE\n",
      "EPOCH 97 COMPLETE\n",
      "EPOCH 98 COMPLETE\n",
      "EPOCH 99 COMPLETE\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(MAX_EPOCHS):\n",
    "    batch = get_batch(params, current_batch_counter)\n",
    "\n",
    "    _, loss_value = sess.run([train_opt, loss],\n",
    "                             feed_dict={\n",
    "                                input: batch['inputs'],\n",
    "                                seq_len: batch['seq_lengths'],\n",
    "                                targets: sparse_tuple_from(batch['targets']),\n",
    "                                rnn_keep_prob: DROPOUT,\n",
    "                            })\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        # VALIDATION\n",
    "        print ('Loss value at epoch ' + str(epoch) + ':' + str(loss_value))\n",
    "        print ('Validating...')\n",
    "\n",
    "#         validation_batch, validation_size = getValidation(params, validation_dict)\n",
    "#         print(\"exit\")\n",
    "#         val_idx = 0\n",
    "        \n",
    "#         val_ed = 0\n",
    "#         val_len = 0\n",
    "#         val_count = 0\n",
    "            \n",
    "#         while val_idx < validation_size:\n",
    "#             mini_batch_feed_dict = {\n",
    "#                 inputs: validation_batch['inputs'][val_idx:val_idx+params['batch_size']],\n",
    "#                 seq_len: validation_batch['seq_lengths'][val_idx:val_idx+params['batch_size']],\n",
    "#                 rnn_keep_prob: 1.0            \n",
    "#             }            \n",
    "                        \n",
    "            \n",
    "#             prediction = sess.run(decoded,\n",
    "#                                   mini_batch_feed_dict)\n",
    "    \n",
    "#             str_predictions = sparse_tensor_to_strs(prediction)\n",
    "    \n",
    "\n",
    "#             for i in range(len(str_predictions)):\n",
    "#                 ed = edit_distance(str_predictions[i], validation_batch['targets'][val_idx+i])\n",
    "#                 val_ed = val_ed + ed\n",
    "#                 val_len = val_len + len(validation_batch['targets'][val_idx+i])\n",
    "#                 val_count = val_count + 1\n",
    "                \n",
    "#             val_idx = val_idx + params['batch_size']\n",
    "#             print(val_idx)\n",
    "    \n",
    "#        print ('[Epoch ' + str(epoch) + '] ' + str(1. * val_ed / val_count) + ' (' + str(100. * val_ed / val_len) + ' SER) from ' + str(val_count) + ' samples.')        \n",
    "        print ('Saving the model...')\n",
    "        saver.save(sess,SAVE_MODEL,global_step=epoch)\n",
    "        print ('------------------------------')\n",
    "        \n",
    "    print(\"EPOCH \" +str(epoch) + \" COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check for GPU availability\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = \"/models/semantic_model.meta\"\n",
    "primus_path = \"/primus_data/\"\n",
    "#path_to_model = \"/models/semantic_1-1-90.meta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ctc_predict = \"/tf-end-to-end/ctc_predict.py\"\n",
    "python_path = \"/usr/bin/python\"\n",
    "vocabulary_path = \"/tf-end-to-end/Data/vocabulary_semantic.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python ctc_predict.py -image Data/Example/000051652-1_2_1.png -model Models/semantic_model.meta -vocabulary Data/vocabulary_semantic.txt\n",
    "def make_prediction(path_input):\n",
    "    command = python_path + \" \" +path_to_ctc_predict + \" -image \" +path_input +\" -model \"+path_to_model +\" -vocabulary \" +vocabulary_path + \" > /log.txt\"\n",
    "    subprocess.Popen(command, shell=True)\n",
    "    output = subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_semantic(s1, s2):\n",
    "    s1 = s1.split('\\t')\n",
    "    s2 = s2.split('\\t')\n",
    "    a = 0\n",
    "    \n",
    "    for i in range(min(len(s1), len(s2))):\n",
    "        if(s1[i].strip() == s2[i].strip()):\n",
    "            a = a + 1\n",
    "    \n",
    "    \n",
    "    a /= min(len(s1), len(s2))\n",
    "    return a\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "hit\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#run accross the test set (n = 8767)\n",
    "n = 1\n",
    "accuracy = 0\n",
    "random.shuffle(validation_list)\n",
    "for i in range(n):\n",
    "    #make the prediction\n",
    "    fp = primus_path + validation_list[i] +\"/\" +validation_list[i] + \".png\"\n",
    "    make_prediction(fp)\n",
    "    f = open(\"/log.txt\", \"r\")\n",
    "    o = open(primus_path + validation_list[i] +\"/\" +validation_list[i] + \".semantic\")\n",
    "    s1 = f.read()\n",
    "    s2 = o.read()\n",
    "    accuracy += compare_semantic(s1, s2)\n",
    "    print(accuracy)\n",
    "    f.close()\n",
    "    o.close()\n",
    "    \n",
    "#normalize the accuracy\n",
    "accuracy = accuracy / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
